{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Neural Ordinary Differential Equations \\- MSur](https://msurtsukov.github.io/Neural-ODE/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.  , 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 ,\n",
       "       0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 , 0.21,\n",
       "       0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 , 0.31, 0.32,\n",
       "       0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 , 0.41, 0.42, 0.43,\n",
       "       0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5 , 0.51, 0.52, 0.53, 0.54,\n",
       "       0.55, 0.56, 0.57, 0.58, 0.59, 0.6 , 0.61, 0.62, 0.63, 0.64, 0.65,\n",
       "       0.66, 0.67, 0.68, 0.69, 0.7 , 0.71, 0.72, 0.73, 0.74, 0.75, 0.76,\n",
       "       0.77, 0.78, 0.79, 0.8 , 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87,\n",
       "       0.88, 0.89, 0.9 , 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98,\n",
       "       0.99, 1.  ])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "numpy.linspace(0., 1., 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function numpy.cumsum(a, axis=None, dtype=None, out=None)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.diff(numpy.linspace(0., 1., 101))\n",
    "numpy.cumsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.71826824, 5.43653647, 8.15480471])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "def euler_odesolver(z0, t0, t1, f):\n",
    "    t_array = numpy.linspace(t0, t1, 100001)\n",
    "    z = z0\n",
    "    for t, h in zip(t_array[1:], numpy.diff(t_array)):\n",
    "        z = z + h*f(z, t)\n",
    "    return z\n",
    "euler_odesolver(numpy.array([1,2,3]), 0, 1, lambda x,t : x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.71828183, 5.43656366, 8.15484549])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.e * numpy.array([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "class ODEF(nn.Module):\n",
    "    def forward_with_grad(self, z, t, a):\n",
    "        \"\"\"\n",
    "        Compute f, adf/dz, adf/dp(parameter:theta) & adf/dt\n",
    "        Args:\n",
    "            - z:\n",
    "                - shape:(batch_size, )\n",
    "            - t:\n",
    "            - a: adjoint\n",
    "                - the length of `graph_outputs` should match the length of `outputs`\n",
    "                    >[Automatic differentiation package \\- torch\\.autograd — PyTorch master documentation](https://pytorch.org/docs/stable/autograd.html)\n",
    "        \"\"\"\n",
    "        batch_size = z.shape[0]\n",
    "        \n",
    "        #calc forward propagation\n",
    "        out = self.forward(z, t)\n",
    "        #calc backward propagation\n",
    "        adfdz, adfdt, *adfdp = torch.autograd.grad(\n",
    "            (out,),\n",
    "            (z, t) + tuple(self.parameters()),\n",
    "            grad_outputs=(a,),\n",
    "            allow_unused=True,\n",
    "            retain_graph=True,\n",
    "        )\n",
    "        #expand adfdp\n",
    "        #[adfdps_of_parameter1, adfdps_of_parameter2,...] -> [*adfdp_in_batch1, *adfdp_in_batch2,...]/batchsize\n",
    "        adfdp = torch.cat([grad_about_a_parameter.flatten() for grad_about_a_parameter in adfdp]).unsqueeze(0)\n",
    "        adfdp = adfdp.expand(batch_size, -1)#-1 should be the number of parameters\n",
    "        #expand adfdt\n",
    "        adfdt = adfdt.expand(batch_size, 1) / batch_size\n",
    "\n",
    "        return out, adfdz, adfdt, adfdp\n",
    "\n",
    "    def flatten_parameters(self):\n",
    "        return torch.cat([p.flatten() for p in self.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7207, 1.5835, 0.9731],\n",
       "        [0.7207, 1.5835, 0.9731],\n",
       "        [0.7207, 1.5835, 0.9731]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([torch.randn(1,3)]*3,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">We have to separate it from main `torch.nn.Module` because custom backward function can’t be implemented inside Module, but can be implemented inside `torch.autograd.Function`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-16-0c15c444c758>, line 22)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-0c15c444c758>\"\u001b[0;36m, line \u001b[0;32m22\u001b[0m\n\u001b[0;31m    for i_t\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class ODEAdjoint(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Custom autograd function for neural ode\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(context, z0, t, flat_parameters, func):\n",
    "        \"\"\"\n",
    "        underconstruction\n",
    "        Args:\n",
    "            context:\n",
    "            z0:\n",
    "            t:\n",
    "            flat_parameters:return of `ODEF.flatten_parameters()`\n",
    "            func: ODEF\n",
    "        \"\"\"\n",
    "        assert isinstance(func, ODEF)#To avoid emerge type error only in backword()\n",
    "        \n",
    "        batch_size, *z_shape = z0.size()\n",
    "        #???\n",
    "        with torch.no_grad():\n",
    "            z = torch.zeros(len(t), batch_size, *z_shape).to(z0)#make same type array & use same device\n",
    "            for i_t\n",
    "        #save states for backward\n",
    "        ctx.func = func\n",
    "        ctx.save_for_backward(\n",
    "            t,\n",
    "            z.clone(),#why take copy?\n",
    "            flat_parameters,\n",
    "        )\n",
    "        \n",
    "        return z\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(context, dLdz):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dLdz:\n",
    "                shape: len(t), batch_size, *z_shape\n",
    "        \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
